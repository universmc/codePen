"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.pythonSnippets = exports.snippetDocumentQuestionAnswering = exports.snippetTextToAudio = exports.snippetTabular = exports.snippetTextToImage = exports.snippetFile = exports.snippetBasic = exports.snippetZeroShotImageClassification = exports.snippetZeroShotClassification = exports.snippetConversational = void 0;
exports.getPythonInferenceSnippet = getPythonInferenceSnippet;
exports.hasPythonInferenceSnippet = hasPythonInferenceSnippet;
const common_js_1 = require("./common.js");
const inputs_js_1 = require("./inputs.js");
const snippetImportInferenceClient = (model, accessToken) => `from huggingface_hub import InferenceClient
client = InferenceClient("${model.id}", token="${accessToken || "{API_TOKEN}"}")
`;
const snippetConversational = (model, accessToken, opts) => {
    const streaming = opts?.streaming ?? true;
    const exampleMessages = (0, inputs_js_1.getModelInputSnippet)(model);
    const messages = opts?.messages ?? exampleMessages;
    const messagesStr = (0, common_js_1.stringifyMessages)(messages, { attributeKeyQuotes: true });
    const config = {
        ...(opts?.temperature ? { temperature: opts.temperature } : undefined),
        max_tokens: opts?.max_tokens ?? 500,
        ...(opts?.top_p ? { top_p: opts.top_p } : undefined),
    };
    const configStr = (0, common_js_1.stringifyGenerationConfig)(config, {
        indent: "\n\t",
        attributeValueConnector: "=",
    });
    if (streaming) {
        return [
            {
                client: "huggingface_hub",
                content: `from huggingface_hub import InferenceClient

client = InferenceClient(api_key="${accessToken || "{API_TOKEN}"}")

messages = ${messagesStr}

stream = client.chat.completions.create(
    model="${model.id}", 
	messages=messages, 
	${configStr},
	stream=True
)

for chunk in stream:
    print(chunk.choices[0].delta.content, end="")`,
            },
            {
                client: "openai",
                content: `from openai import OpenAI

client = OpenAI(
	base_url="https://api-inference.huggingface.co/v1/",
	api_key="${accessToken || "{API_TOKEN}"}"
)

messages = ${messagesStr}

stream = client.chat.completions.create(
    model="${model.id}", 
	messages=messages, 
	${configStr},
	stream=True
)

for chunk in stream:
    print(chunk.choices[0].delta.content, end="")`,
            },
        ];
    }
    else {
        return [
            {
                client: "huggingface_hub",
                content: `from huggingface_hub import InferenceClient

client = InferenceClient(api_key="${accessToken || "{API_TOKEN}"}")

messages = ${messagesStr}

completion = client.chat.completions.create(
    model="${model.id}", 
	messages=messages, 
	${configStr}
)

print(completion.choices[0].message)`,
            },
            {
                client: "openai",
                content: `from openai import OpenAI

client = OpenAI(
	base_url="https://api-inference.huggingface.co/v1/",
	api_key="${accessToken || "{API_TOKEN}"}"
)

messages = ${messagesStr}

completion = client.chat.completions.create(
    model="${model.id}", 
	messages=messages, 
	${configStr}
)

print(completion.choices[0].message)`,
            },
        ];
    }
};
exports.snippetConversational = snippetConversational;
const snippetZeroShotClassification = (model) => ({
    content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
    "inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
    "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
})`,
});
exports.snippetZeroShotClassification = snippetZeroShotClassification;
const snippetZeroShotImageClassification = (model) => ({
    content: `def query(data):
	with open(data["image_path"], "rb") as f:
		img = f.read()
	payload={
		"parameters": data["parameters"],
		"inputs": base64.b64encode(img).decode("utf-8")
	}
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
    "image_path": ${(0, inputs_js_1.getModelInputSnippet)(model)},
    "parameters": {"candidate_labels": ["cat", "dog", "llama"]},
})`,
});
exports.snippetZeroShotImageClassification = snippetZeroShotImageClassification;
const snippetBasic = (model) => ({
    content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output = query({
	"inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})`,
});
exports.snippetBasic = snippetBasic;
const snippetFile = (model) => ({
    content: `def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.post(API_URL, headers=headers, data=data)
    return response.json()

output = query(${(0, inputs_js_1.getModelInputSnippet)(model)})`,
});
exports.snippetFile = snippetFile;
const snippetTextToImage = (model, accessToken) => [
    {
        client: "huggingface_hub",
        content: `${snippetImportInferenceClient(model, accessToken)}
# output is a PIL.Image object
image = client.text_to_image(${(0, inputs_js_1.getModelInputSnippet)(model)})`,
    },
    {
        client: "requests",
        content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content
image_bytes = query({
	"inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})

# You can access the image with PIL.Image for example
import io
from PIL import Image
image = Image.open(io.BytesIO(image_bytes))`,
    },
];
exports.snippetTextToImage = snippetTextToImage;
const snippetTabular = (model) => ({
    content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content
response = query({
	"inputs": {"data": ${(0, inputs_js_1.getModelInputSnippet)(model)}},
})`,
});
exports.snippetTabular = snippetTabular;
const snippetTextToAudio = (model) => {
    // Transformers TTS pipeline and api-inference-community (AIC) pipeline outputs are diverged
    // with the latest update to inference-api (IA).
    // Transformers IA returns a byte object (wav file), whereas AIC returns wav and sampling_rate.
    if (model.library_name === "transformers") {
        return {
            content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content

audio_bytes = query({
	"inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})
# You can access the audio with IPython.display for example
from IPython.display import Audio
Audio(audio_bytes)`,
        };
    }
    else {
        return {
            content: `def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
audio, sampling_rate = query({
	"inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})
# You can access the audio with IPython.display for example
from IPython.display import Audio
Audio(audio, rate=sampling_rate)`,
        };
    }
};
exports.snippetTextToAudio = snippetTextToAudio;
const snippetDocumentQuestionAnswering = (model) => ({
    content: `def query(payload):
 	with open(payload["image"], "rb") as f:
  		img = f.read()
		payload["image"] = base64.b64encode(img).decode("utf-8")  
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
    "inputs": ${(0, inputs_js_1.getModelInputSnippet)(model)},
})`,
});
exports.snippetDocumentQuestionAnswering = snippetDocumentQuestionAnswering;
exports.pythonSnippets = {
    // Same order as in tasks/src/pipelines.ts
    "text-classification": exports.snippetBasic,
    "token-classification": exports.snippetBasic,
    "table-question-answering": exports.snippetBasic,
    "question-answering": exports.snippetBasic,
    "zero-shot-classification": exports.snippetZeroShotClassification,
    translation: exports.snippetBasic,
    summarization: exports.snippetBasic,
    "feature-extraction": exports.snippetBasic,
    "text-generation": exports.snippetBasic,
    "text2text-generation": exports.snippetBasic,
    "image-text-to-text": exports.snippetConversational,
    "fill-mask": exports.snippetBasic,
    "sentence-similarity": exports.snippetBasic,
    "automatic-speech-recognition": exports.snippetFile,
    "text-to-image": exports.snippetTextToImage,
    "text-to-speech": exports.snippetTextToAudio,
    "text-to-audio": exports.snippetTextToAudio,
    "audio-to-audio": exports.snippetFile,
    "audio-classification": exports.snippetFile,
    "image-classification": exports.snippetFile,
    "tabular-regression": exports.snippetTabular,
    "tabular-classification": exports.snippetTabular,
    "object-detection": exports.snippetFile,
    "image-segmentation": exports.snippetFile,
    "document-question-answering": exports.snippetDocumentQuestionAnswering,
    "image-to-text": exports.snippetFile,
    "zero-shot-image-classification": exports.snippetZeroShotImageClassification,
};
function getPythonInferenceSnippet(model, accessToken, opts) {
    if (model.tags.includes("conversational")) {
        // Conversational model detected, so we display a code snippet that features the Messages API
        return (0, exports.snippetConversational)(model, accessToken, opts);
    }
    else {
        let snippets = model.pipeline_tag && model.pipeline_tag in exports.pythonSnippets
            ? exports.pythonSnippets[model.pipeline_tag]?.(model, accessToken) ?? { content: "" }
            : { content: "" };
        snippets = Array.isArray(snippets) ? snippets : [snippets];
        return snippets.map((snippet) => {
            return {
                ...snippet,
                content: snippet.content.includes("requests")
                    ? `import requests

API_URL = "https://api-inference.huggingface.co/models/${model.id}"
headers = {"Authorization": ${accessToken ? `"Bearer ${accessToken}"` : `f"Bearer {API_TOKEN}"`}}

${snippet.content}`
                    : snippet.content,
            };
        });
    }
}
function hasPythonInferenceSnippet(model) {
    return !!model.pipeline_tag && model.pipeline_tag in exports.pythonSnippets;
}
