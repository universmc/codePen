const fs = require('fs');
const Groq = require('groq-sdk');
const axios = require('axios');

// Initialize the SDKs with API keys
const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });
// Define the prompts and roles for each model
const ŒúœáœáœáŒÆŒª = "œáŒπœáŒ±ŒúŒª"
const œáŒπœáŒ±ŒªŒπ = `"${ŒúœáœáœáŒÆŒª}"`
const ŒúŒπœáŒ±ŒÆŒª = "œáŒπœáŒ±ŒªŒπ"
const œáŒπœáŒ±ŒúŒª = `"${ŒúŒπœáŒ±ŒÆŒª}"`

const encoding = `"${œáŒπœáŒ±ŒªŒπ}:/${œáŒπœáŒ±ŒúŒª}"`

const mission = `"${encoding}"`

const promptingMC = {
  mission: `"perform tasks like lemmatization and Tokenizer Timestamp, encryption > œáŒπœáŒ±ŒªŒπ:/œáŒπœáŒ±ŒªŒπ" and potentially >> (e.g., "encoding") ${mission}.`,
  context: "Enhancing text generation using MC.",
  role: "AI Language Expert",
  tasks: [`"Text Generation", "Context Understanding","${encoding} > ${ŒúŒπœáŒ±ŒÆŒª}"`],
  expectedOutcome: "High-quality, coherent text output."
};

const promptingGemini = {
  mission: `"perform tasks like lemmatization and Tokenizer Timestamp, encryption > œáŒπœáŒ±ŒªŒπ:/œáŒπœáŒ±ŒªŒπ" and potentially >> (e.g., "encoding") ${mission}.`,
  context: "Utilizing GANs for data refinement.",
  role: "AI Generalist",
  tasks: ["Data Generation", "Dynamic Modeling"],
  expectedOutcome: "Improved and adaptable data sets."
};

// Execute the MC model prompting
async function executePromptingMC() {
  try {
    console.log("Starting MC content generation task...");
    const chatCompletion = await groq.chat.completions.create({
      messages: [
        { role: "system", content: `INITIALISATIUON DE LA ${mission}` },
        { role: "assistant", content: JSON.stringify(promptingMC) },
        { role: "user", content: `G√©n√®re un Pr√©sentation d√©taill√© la ${mission} pour le develloppement de ${promptingGemini} pour comprendre le (prompt --engine) a l'aide d'EMOJI int√©lliget, avec les points cl√©s abord√©s, les r√©sum√©s des sujets et les prochaines √©tapes. Int√©gr√© les üíª emoji's int√©lligent & associ√©s √† la pr√©sentation`}
      ],
      model: "gemma2-9b-it",
      temperature: 0.7
    });

    const content = chatCompletion.choices[0]?.message?.content;
    const filePath = `output/œáŒπœáŒ±ŒªŒπ_${Date.now()}.md`;
    fs.writeFileSync(filePath, content || "No content generated.");
    console.log(`MC content saved to ${filePath}`);
  } catch (error) {
    console.log("Error executing MC prompting:", error);
  }
}

// Execute the Gemini model prompting
async function executePromptingGemini() {
  try {
    console.log("Starting Gemini data generation task...");
    const chatCompletion = await groq.chat.completions.create({
      messages: [
        { role: "system", content: `INITIALISATIUON DE LA ${mission}` },
        { role: "assistant", content: JSON.stringify(promptingGemini) },
        { role: "user", content: `G√©n√®re un Pr√©sentation d√©taill√© la ${mission} pour le develloppement de ${promptingMC} pour comprendre le (prompt --engine) a l'aide d'EMOJI int√©lliget, avec les points cl√©s abord√©s, les r√©sum√©s des sujets et les prochaines √©tapes. Int√©gr√© les üíª emoji's int√©lligent & associ√©s √† la pr√©sentation`}
      ],
      model: "gemma2-9b-it",
      temperature: 0.6
    });

    const content = chatCompletion.choices[0]?.message?.content;
    const filePath = `output/œáŒπœáŒ±Œª_${Date.now()}.md`;
    fs.writeFileSync(filePath, content || "No content generated.");
    console.log(`Gemini content saved to ${filePath}`);
  } catch (error) {
    console.log("Error executing Gemini prompting:", error);
  }
}

// Main function to execute both models
async function main() {
  await executePromptingMC();
  await executePromptingGemini();
  
  console.log("Model comparison completed. Check output files for results.");
}

main().catch(console.error);
